{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `computational_analysis_of_big_data_2018_spring` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thursday, February 1, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "This week is about getting data from the big ol' Internet, with the Wikipedia as our guinea pig. The main task today is to retrieve the Wikipedia pages of **all Marvel characters** using the MediaWiki **API**. There are three parts to this exercise.\n",
    "\n",
    "* Learn the basics of how to retrieve data from Wiki sites using the MediaWiki API\n",
    "* Download all Marvel character Wikipedia articles\n",
    "* Explore the data\n",
    "\n",
    "The data you acquire today, you will be working with for the remainder of the course. Try to get as far as possible, structure the data nicely and write your code so that it makes sense to you in the coming weeks.\n",
    "\n",
    "Also, there's an **important practice** you should start getting used to—which matters when we grade assignments. \n",
    "1. Openly reflect on how you solve a problem. It can be code comments, or markup below/above the code cell, just as long as you share your thoughts. \n",
    "2. Comment on your results, discussing:\n",
    "    * Whether they make sense\n",
    "    * If they look somewhat as you expected, and if not, what the reasons for this difference may be\n",
    "    * What—interesting or not—insight they reveal about the given system you analyze\n",
    "    \n",
    "    *Note: of course you can't always say something profound about every little thing, so rest assured, I will only expect explanations in your assignments when *it makes sense* that there should be one.*\n",
    "\n",
    "**Feedback:** Send me anonymous feedback about the exercises, lectures and course in general at http://ulfaslak.com/vent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use an API?** You could just go ahead and scrape the HTML from a Wikipedia page as simple as:\n",
    "\n",
    "    import requests as rq\n",
    "    rq.get(\"https://en.wikipedia.org/wiki/Batman\").text\n",
    "    \n",
    "Well... to navigate data in XML format is not always easy. Therefore, MediaWiki offers its users direct use of its API. To load the MediaWiki markup using the API, one would do something like:\n",
    "\n",
    "    rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content\").json()\n",
    "    \n",
    "This returns a JSON object inside which you can find all sorts of information about the page, including the latest revision of the Batman page markup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful code to display JSON object as a tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-27T14:42:46.309956Z",
     "start_time": "2017-09-27T14:42:45.976698Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchcomplete : unicode - 0\n",
      "query\n",
      "    pages\n",
      "        4335\n",
      "            ns : int - 1\n",
      "            pageid : int - 4\n",
      "            revisions : list - 142273\n",
      "            title : unicode - 6\n"
     ]
    }
   ],
   "source": [
    "def print_json_tree(d, indent=0):\n",
    "    \"\"\"Print tree of keys in JSON object.\n",
    "    \n",
    "    Prints the different levels of nested keys in a JSON object. When there\n",
    "    are no more dictionaries to key into, prints objects type and byte-size.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    d : dict\n",
    "    \"\"\"\n",
    "    for key, value in d.iteritems():\n",
    "        print '    ' * indent + unicode(key),\n",
    "        if isinstance(value, dict):\n",
    "            print; print_json_tree(value, indent+1)\n",
    "        else:\n",
    "            print \":\", str(type(d[key])).split(\"'\")[1], \"-\", str(len(unicode(d[key])))\n",
    "            \n",
    "# Example\n",
    "import requests as rq\n",
    "data = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content\").json()\n",
    "print_json_tree(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Learn to access Wikipedia data with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how Wikipedia markup works .You'll need to know a bit about formatting of MediaWiki pages so that you can parse the markup that you retrieve from wikipedia. See http://www.mediawiki.org/wiki/Help:Formatting. In particular, look into how links work and how tables work and make sure you can answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.0.1**: How do you link to another Wikipedia page from within a Wikipedia-page, using the wikimedia markup? Write down a simple example that links to a specific section in another page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would use double brackets like this [[Page name here]] with the page name inside. Here is an example: \n",
    "[[Puppies#Types]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.0.2**: Can you create a simple table with the same content as the one below, using wikimedia markup?\n",
    "\n",
    ">| True Positive  | False Positive |\n",
    "| -------------- |:--------------:|\n",
    "| False Negative | True Negative  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{|\n",
    " | -\n",
    " | True Positive\n",
    " | False Positive\n",
    " | -\n",
    " | False Negative\n",
    " | True Negative\n",
    " |}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table has the same content but a different format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.0.3**: Figure out how to download pages from Wikipedia. Familiarize yourself with [the API](http://www.mediawiki.org/wiki/API:Main_page) and learn how to extract the markup. The API query that returns the markup of the Batman page is:\n",
    "    \n",
    ">`api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content`\n",
    "\n",
    ">1. Explain the structure of this query. What are the parameters and arguments and what do they mean? What happens if you remove `rvprop=content`?\n",
    "2. Download the Batman page data from the API. Extract the markup from the JSON object and save it to a file called \"batman.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After the question mark, there are a series of query parameters and values for them. This allows the user to specify what he/she wants to get, what format they want to get it, etc. Format specifies how the data should be returned. Action specifies what we want the API to do. Query means that we basically just want to get the information. Titles specifies what pages we want to get information from. Here, we just want to get the Batman page. Properties specifies what information we want to get from the page. Here, we want the information about the revisions. rvprop=content specifies what information to get from the most recent revision. Here we just want the content of the page. If you remove rvprop=content, it will not return any of the wikipedia page's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import json\n",
    "url = \"https://en.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content\"\n",
    "data = rq.get(url).json()\n",
    "bfile = open('batman.txt', 'w+')\n",
    "json.dump(data, bfile)\n",
    "bfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Get data (main part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a good part of this course we will be working with data from Wikipedia. Today, your objective is to crawl a large dataset with good and bad characters from the Marvel characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes' to get the characters in each category.\n",
    "1. How many superheroes are there? How many supervillains?\n",
    "2. How many characters are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-735a8ce035f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0murlh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes&cmlimit=500&prop=revisions&rvprop=content&cmcontinue=\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mconth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdatah\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mheroes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatah\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorymembers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rq' is not defined"
     ]
    }
   ],
   "source": [
    "conth = \"0\"\n",
    "contv = \"0\"\n",
    "heroes = list()\n",
    "villains = list()\n",
    "# Loop through each hero and get the data for the heroes\n",
    "while True:\n",
    "    urlh = \"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes&cmlimit=500&prop=revisions&rvprop=content&cmcontinue=\"+ conth\n",
    "    datah = rq.get(urlh).json()\n",
    "    heroes.extend(datah['query']['categorymembers'])\n",
    "    try:\n",
    "        conth = datah['continue']['cmcontinue']\n",
    "    except:\n",
    "        break\n",
    "# Do the same for villains        \n",
    "while True:\n",
    "    urlv = \"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_supervillains&cmlimit=500&prop=revisions&rvprop=content&cmcontinue=\"+ contv\n",
    "    datav = rq.get(urlv).json()\n",
    "    villains.extend(datav['query']['categorymembers'])\n",
    "    try:\n",
    "        contv = datav['continue']['cmcontinue']\n",
    "    except:\n",
    "        break\n",
    "# Get the heroes and villains and ambiguous's names from the data\n",
    "heroes = [x['title'] for x in heroes]\n",
    "villains = [x['title'] for x in villains]\n",
    "ambiguous = set(heroes).intersection(set(villains))\n",
    "print \"Heroes: \" + str(len(heroes))\n",
    "print \"Villains: \" +str(len(villains))\n",
    "print \"Both heroes and villains: \" + str(len(ambiguous))\n",
    "print \"Jacquard Similarity: \" + str(float(len(set(heroes).intersection(set(villains))))/float(len(set(heroes).union(set(villains)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.2**: Using this list you now want to download all data you can about each character. However, because this is potentially Big Data, you cannot store it your computer's memory. Therefore, you have to store it in your harddrive somehow. \n",
    "* Create three folders on your computer, one for *heroes*, one for *villains*, and one for *ambiguous*.\n",
    "* For each character, download the markdown on their pages and save in a new file in the corresponding hero/villain/ambiguous folder.\n",
    "\n",
    ">*Hint: Some of the characters have funky names. The first problem you may encounter is problems with encoding. To solve that you can call `.encode('utf-8')` on your markup string. Another problem you may encounter is that characters have a slash in their names. This, you should just replace with some other meaningful character.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with heroes\n",
      "Done with Villains\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Only heroes\n",
    "for hero in heroes:\n",
    "    if hero not in ambiguous:\n",
    "        data = rq.get(\"https://en.wikipedia.org/wiki/\" + hero.encode('utf-8')).text.encode('utf-8')\n",
    "        hero = hero.replace(\"/\", \"~\")\n",
    "        hero = hero.replace(\"\\\\\", \"~\")\n",
    "        pathname = \"./heroes/\" + hero.encode('utf-8')\n",
    "        f = open(pathname, \"w+\") \n",
    "        f.write(data)\n",
    "        f.close()\n",
    "print \"Done with heroes\"        \n",
    "for villain in villains:\n",
    "    if villain not in ambiguous:\n",
    "        data = rq.get(\"https://en.wikipedia.org/wiki/\" + villain.encode('utf-8')).text.encode('utf-8')\n",
    "        villain = villain.replace(\"/\", \"~\")\n",
    "        villain = villain.replace(\"\\\\\", \"~\")\n",
    "        pathname = \"./villains/\" + villain.encode('utf-8')\n",
    "        f = open(pathname, \"w+\") \n",
    "        f.write(data)\n",
    "        f.close()\n",
    "print \"Done with Villains\"        \n",
    "for amb in ambiguous:\n",
    "    data = rq.get(\"https://en.wikipedia.org/wiki/\" + amb.encode('utf-8')).text.encode('utf-8')\n",
    "    amb = amb.replace(\"/\", \"~\")\n",
    "    amb = amb.replace(\"\\\\\", \"~\")\n",
    "    pathname = \"./ambiguous/\" + amb.encode('utf-8')\n",
    "    f = open(pathname, \"w+\") \n",
    "    f.write(data)\n",
    "    f.close()\n",
    "print \"Done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character, and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    "\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. Use `plt.hist` on a list of page lengths, with the argument `normed=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Doing this by line\n",
    "hero_length = dict()\n",
    "vill_length = dict()\n",
    "ambi_length = dict()\n",
    "for file_name in os.listdir(\"./heroes\"):\n",
    "    s = \"./heroes/\" + file_name\n",
    "    with open(s) as f:\n",
    "        lines = 0\n",
    "        for line in f:\n",
    "            lines += 1\n",
    "    hero_length[file_name] = lines\n",
    "\n",
    "for file_name in os.listdir(\"./villains\"):\n",
    "    s = \"./villains/\" + file_name\n",
    "    with open(s) as f:\n",
    "        lines = 0\n",
    "        for line in f:\n",
    "            lines += 1\n",
    "    vill_length[file_name] = lines\n",
    "\n",
    "for file_name in os.listdir(\"./ambiguous\"):\n",
    "    s = \"./ambiguous/\" + file_name\n",
    "    with open(s) as f:\n",
    "        lines = 0\n",
    "        for line in f:\n",
    "            lines += 1\n",
    "    ambi_length[file_name] = lines\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0be99c076349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "sorted(, key=A.get, reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: We are interested in knowing if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "* For each class, visualize the amount of characters introduced over time. You choose how you want to visualize this data, but please comment on your choice. Also comment on the outcome of your analysis.\n",
    "\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*\n",
    "\n",
    ">*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alliances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.4**: In this exercise you want to find out what the biggest alliances in the Marvel universe are. The data you need for doing this is in the *alliances*-field of the markup of each character. Below I suggest steps you can take to solve the problem if you get stuck.\n",
    "* Write a regex that extracts the *alliances*-field of a character's markup.\n",
    "* Write a regex that extracts each team from the *alliance*-field.\n",
    "* Count the number of members for each team (hint: use a `defaultdict`).\n",
    "* Inspect your team names. Are there any that result from inconsistencies in the information on the pages? How do you deal with this?\n",
    "* **Print the 10 largest alliances and their number of members.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
